---
layout:     post
title:      "从范数到正则化"
subtitle:   "机器学习数学原理补番计划"
date:       2019-05-14 
author:     "Yean"
header-img: "img/post-bg-unix-linux.jpg"
catalog: true
tags:
    - 学习笔记

---

# 从范数到正则化



[范数](https://zh.wikipedia.org/wiki/%E8%8C%83%E6%95%B0)是一个在数学领域中常用的工具，同时也是学习机器学习原理中经常碰到的概念。本文将从范数的定义出发，逐步带你理解其在机器学习中的应用。

首先需要明确的是，范数是一个**函数**，在机器学习中我们通常用它来衡量向量的大小。$$L^p$$范数定义为：

$$
\left \| x \right \|_p = \left ( \sum_{i}\left | x^i \right |^p \right )^{1/p}
$$

### 1.常见的范数

下面简要介绍一些常见的范数，到这一步暂且只需要记住它们的形式。

#### 1.1 $L^2$范数

当$$p = 2$$时，$$L^2$$范数也被称为欧几里得范数，表示从远点出发到向量$$x$$确定的点的欧几里得距离。这个范数在机器学习中应用的非常频繁，我们先记住它的简化表示:$$\left \| x \right \|$$。

$$
\left \| x \right \| = \left ( \sum_{i}\left | x^i \right |^2 \right )^{1/2}
$$

#### 1.2 平方$L^2$范数

顾名思义就是$$L^2$$范数的平方，好处就是它显然比$$L^2$$范数容易计算，可以简单的通过点积$$x^Tx$$计算。

#### 1.3 $L^1$范数

有些情况下平方$$L^2$$范数不是很受欢迎，因为它在原点附近增长得十分缓慢。**有时候区分恰好是零和非零但值很小的元素是很重要的**，这时候就可以使用**各位置斜率相同**的$$L^1$$范数：

$$
\left \| x \right \|_1 =   \sum_{i}\left | x^i \right |
$$

#### 1.4 $L^\infty $范数

$$L^\infty $$范数也被称为最大范数，表示向量中具有最大幅值的元素的绝对值：

$$
\left \| x \right \|_\infty  = \underset{i}{max}\left | x_i \right |
$$


### 2.深度学习中的正则化

#### 2.1偏差(bias)和方差(variance)

在介绍深度学习中的正则化之前，我们先要从机器学习的场景出发思考，**是什么问题促使我们需要用正则化这个工具呢？​**

偏差和方差通常可以用来判断模型拟合数据的情况，看下面这张图，$$\bigcirc $$和$\times $代表两种不同的样本点。

第一个坐标系中由于分类器接近于线性，拟合数据的能力比较差，表现出**欠拟合**，对应**高偏差high bias**；第三个坐标系对于训练数据**过拟合**，对应**高方差high variance**；而中间的坐标系则是恰到好处的，我们比较希望得到的泛化能力较强的模型。

![](https://upload-images.jianshu.io/upload_images/1083955-b10130b80ea9ae5a.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

因此：

* if high bias:{更大的网络/更长的训练时长/（更合适的算法）}
* if high variance:{更多的数据/**正则化**/（更合适的算法）}

我们可以将正则化理解为“对学习算法的修改——目的是为了**减少泛化误差**，以偏差的增加换取方差的减少，从而抑制过拟合。”

#### 2.2正则化如何抑制过拟合

我们将目标函数定义为$J(\theta ;X,y)$,正则化后的目标函数为$\tilde{J}(\theta ;X,y)$,$\theta = w + b$。通常**只对权重$w$做正则化惩罚**而不针对偏置项$b$,为了简单起见，我们假设没有偏置项：

$$
\tilde{J}(w ;X,y) = J(w ;X,y) + \alpha 正则项
$$

$\alpha \in [0,\infty)$是权衡正则化强度的超参。

##### 2.2.1 $L^2$正则化

$L^2$范数正则化也被称为权重衰减，这时

$$
正则项 = \frac{1}{2}\left \| w \right \|_{2}^{2}
$$

带入正则化后的目标函数

$$
\tilde{J}(w ;X,y) = J(w ;X,y) +  \frac{\alpha}{2}\left \| w \right \|_{2}^{2}
$$

从上文1.2 平方$$L^2$$范数的介绍中得到$\left \| w \right \|_{2}^{2} = w^Tw$，因此

$$
\tilde{J}(w ;X,y) = J(w ;X,y) +  \frac{\alpha}{2}w^Tw
$$

与之对应的梯度为：

$$
\bigtriangledown _w\tilde{J}(w ;X,y) = \bigtriangledown _wJ(w ;X,y) + \alpha w
$$

使用梯度下降更新权重，$$\epsilon $$为学习率：

$$
w\leftarrow w - \epsilon (\bigtriangledown _wJ(w ;X,y) + \alpha w)
$$

即：

$$
w\leftarrow (1-\epsilon \alpha)w -  \epsilon (\bigtriangledown _wJ(w ;X,y) )
$$
可以看出**加入权重衰减后会引起学习规则的修改**，在每步梯度更新之前都会先收缩权重向量——将权重向量前边乘上一个小于1的权重因子$(1-\epsilon \alpha)$，也就是说，正则化惩罚系数$\alpha $升高会将权重$w$拉向0。可以从两个角度进一步理解一下这个操作：

a.在神经网络中，当一些权重趋近于0时，则可以理解为去掉了一些逻辑单元，简化后的网络虽然小但深度很大。从而将高方差的模型往高偏差的方向拉，直到获取一个恰到好处的模型。![](https://upload-images.jianshu.io/upload_images/1083955-86856006464d63cc.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

b.假设神经网络每层的激活函数为$g(z) = tanh(z)$，$z = w^{[l] }\varepsilon^{[l-1] }+b$，正则化惩罚系数$\alpha 升高 \rightarrow w减小 \rightarrow z 减小 $，从而使得$g(z)$从①③的**非线性状态**区域进入②接近于**线性状态**的区域，导致每层几乎是线性的（线性函数叠加仍然为线性），起到抑制过拟合的效果。![](https://upload-images.jianshu.io/upload_images/1083955-832fefa13ad08bc0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
